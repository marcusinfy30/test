# Responsible-AI-Moderation Model 

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Installation](#installation)
- [Set Configuration Variables](#set-configuration-variables)
- [Models Required](#models-required)
- [Running the Application](#running-the-application)
- [License](#license)
- [Contact](#contact)
  
## Introduction
The **Moderation Model** module acts as a central hub for machine learning models for prompt injection, toxicity, jailbreak, restricted topic, custom theme and refusal checks. It provides the endpoints to utilize the response generated by these models.

## Features
The **Moderation Model** module acts as a wrapper for the traditional AI models we are using for various checks like prompt injection, jailbreak, toxicity etc. 

## Installation
To run the application, first we need to install Python and the necessary packages:

1. Install Python (version >= 3.9) from the [official website](https://www.python.org/downloads/) and ensure it is added to your system PATH.

2. Clone the repository : responsible-ai-ModerationModel:
    ```sh
    git clone <repository-url>
    ```

3. Navigate to the `responsible-ai-ModerationModel` directory:
    ```sh
    cd responsible-ai-ModerationModel
    ```

4. Create a virtual environment:
    ```sh
    python -m venv venv
    ```

5. Activate the virtual environment:
    - On Windows:
        ```sh
        .\venv\Scripts\activate
         ```

6. Go to the `requirements` directory where the `requirement.txt` file is present.
    In the `requirement.txt` file comment the 
    ```sh
    lib/torch-2.2.0+cu118-cp39-cp39-linux_x86_64.whl
    ``` 
    if working in windows as this is for linux and replace 
    ```sh
    lib/
    ```
    with 
    ```sh
    ../lib/
    ``` 
    Download and place the en_core_web_lg-3.5.0-py3-none-any.whl inside the lib folder.
    [en_core_web_lg](https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl)
    and install the requirements:
    ```sh
    pip install -r requirement.txt
    ```
    Install the fastapi library as well, use the following command:
    ```sh
    pip install fastapi
    ```
## Set Configuration Variables
After installing all the required packages, configure the variables necessary to run the APIs.

1. Navigate to the `src` directory:
    ```sh
    cd ..
    ```

2. Locate the `.env` file, which contains keys like the following:

  ```sh
  workers=1
  WORKERS="${workers}"
  # DB_NAME="${dbname}"
  # DB_USERNAME="${username}"
  # DB_PWD="${password}"
  # DB_IP="${ipaddress}"
  # DB_PORT="${port}"
  # MONGO_PATH="mongodb://${DB_USERNAME}:${DB_PWD}@${DB_IP}:${DB_PORT}/"
  # MONGO_PATH= "mongodb://localhost:27017/"
  ```

3. Replace the placeholders with your actual values.

## Models Required
The following models are required to run the application. Download all the model files from the links provided, and place it in the folder name provided.

1. [Prompt Injection](https://huggingface.co/deepset/deberta-v3-base-injection/tree/main)
Files required to download here are : model.safetensors, config.json, tokenizer_config.json, tokenizer.json, special_tokens_map.json.
Name the folder as 'dbertaInjection'.

2. [Restricted Topic](https://huggingface.co/MoritzLaurer/deberta-v3-large-zeroshot-v1/tree/main)
Files required to download here are : pytorch_model.bin, added_tokens.json, config.json, special_tokens_map.json, spm.model, tokenizer.json, tokenizer_config.json.
Name the folder as 'restricted-dberta-large-zeroshot'.
3. [Sentence Transformer Model](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1/tree/main)
Files required to download here are : 1_Pooling folder, pytorch_model.bin, vocal.txt, tokenizer.json, tokenizer_config.json, special_tokens_map.json, sentence_bert_config.json, modules.json, config.json, config_sentence_transformers.json.
Name the folder as 'multi-qa-mpnet-base-dot-v1'.
4. [Detoxify](https://huggingface.co/FacebookAI/roberta-base/tree/main)
Files required to download here are : vocab.json, tokenizer.json, merges.txt, config.json.
Now download the model checkpoint file from this url and keep it under this folder -
[toxic_model_ckpt_file](https://github.com/unitaryai/detoxify/releases/download/v0.3-alpha/toxic_debiased-c7548aa0.ckpt)
Name the folder as 'detoxify'.

Place the above folders in a folder named 'models' in the following way: 'responsible-ai-mm-flask/models'.

In main.py comment the following lines:
from routing.safety_router import img_router
app.register_blueprint(img_router,url_prefix='/rai/v1/raimoderationmodels')


## Running the Application
Once we have completed all the aforementioned steps, we can start the service.

1. Navigate to the `src` directory:

2. Run `main.py` file:
    ```sh
    python main.py
     ```

3. PORT_NO : Use the Port No that is configured in .env file.

   Open the following URL in your browser:
  `http://localhost:<PORT_NO>/rai/v1/raimoderationmodels/docs`


  
## License
The source code for the project is licensed under the MIT license, which you can find in the [LICENSE.txt](LICENSE.txt) file.

## Contact
If you have more questions or need further insights please feel free to connect with us @ Infosysraitoolkit@infosys.com



